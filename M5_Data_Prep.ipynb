{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b1402146",
      "metadata": {
        "id": "b1402146"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import dask.dataframe as dk\n",
        "import calendar\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler,Normalizer\n",
        "from scipy.sparse import csr_matrix,hstack\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import tensorflow as tf\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "id": "RAkLmhfX3COG"
      },
      "id": "RAkLmhfX3COG"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e7c03827",
      "metadata": {
        "id": "e7c03827"
      },
      "outputs": [],
      "source": [
        "calendar_df = pd.read_csv(\"calendar.csv\")\n",
        "train_validation = pd.read_csv(\"sales_train_validation.csv\")\n",
        "sales_df = pd.read_csv(\"sell_prices.csv\")\n",
        "train_evaluation = pd.read_csv(\"sales_train_evaluation.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d_columns = [f'd_{i}' for i in range(1, 1914)]\n",
        "\n",
        "df_final = pd.melt(train_validation,\n",
        "                   id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'],\n",
        "                   value_vars=d_columns,\n",
        "                   var_name=\"d\",\n",
        "                   value_name=\"sales\")\n",
        "\n",
        "d_columns = [f'd_{i}' for i in range(1914, 1942)]\n",
        "\n",
        "df_final_test = pd.melt(train_evaluation,\n",
        "                        id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'],\n",
        "                        value_vars=d_columns,\n",
        "                        var_name=\"d\",\n",
        "                        value_name=\"sales\")\n",
        "\n",
        "for i in range(1942, 1970):\n",
        "    train_evaluation[f'd_{i}'] = 0\n",
        "\n",
        "d_columns = [f'd_{i}' for i in range(1942, 1970)]\n",
        "\n",
        "df_future_data = pd.melt(train_evaluation,\n",
        "                         id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'],\n",
        "                         value_vars=d_columns,\n",
        "                         var_name=\"d\",\n",
        "                         value_name=\"sales\")"
      ],
      "metadata": {
        "id": "p64wluu_XHa1"
      },
      "id": "p64wluu_XHa1",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a87b434e",
      "metadata": {
        "id": "a87b434e"
      },
      "outputs": [],
      "source": [
        "data=df_final.merge(calendar_df,on='d',copy=False)\n",
        "data=data.merge(sales_df,on=[\"store_id\", \"item_id\", \"wm_yr_wk\"],copy=False)\n",
        "data.to_csv('final_dataframe.csv',index=False)\n",
        "\n",
        "data_test=df_final_test.merge(calendar_df,on='d',copy=False)\n",
        "data_test=data_test.merge(sales_df,on=[\"store_id\", \"item_id\", \"wm_yr_wk\"],copy=False)\n",
        "data_test.to_csv('final_dataframe_test.csv',index=False)\n",
        "\n",
        "\n",
        "data_future=df_future_data.merge(calendar_df,on='d',copy=False)\n",
        "data_future=data_future.merge(sales_df,on=[\"store_id\", \"item_id\", \"wm_yr_wk\"],copy=False)\n",
        "data_future.to_csv('final_future_data.csv',index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "225c8a66",
      "metadata": {
        "id": "225c8a66"
      },
      "outputs": [],
      "source": [
        "train = data.copy()\n",
        "test = data_test.copy()\n",
        "final_test = data_future.copy()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_encode = [\n",
        "    'event_name_1', 'event_name_2',\n",
        "    'event_type_1', 'event_type_2',\n",
        "    'item_id', 'dept_id',\n",
        "    'cat_id', 'store_id',\n",
        "    'state_id', 'year'\n",
        "]\n",
        "\n",
        "# Datasets to be transformed\n",
        "datasets = {\n",
        "    'train': train,\n",
        "    'test': test,\n",
        "    'final_test': final_test\n",
        "}\n",
        "\n",
        "# Loop through each column and apply LabelEncoder, then save the encoder\n",
        "for column in columns_to_encode:\n",
        "    lbl = LabelEncoder()\n",
        "\n",
        "    # Fit on train, then transform on all datasets\n",
        "    datasets['train'][column] = lbl.fit_transform(datasets['train'][column])\n",
        "    for dataset_name in ['test', 'final_test']:\n",
        "        datasets[dataset_name][column] = lbl.transform(datasets[dataset_name][column])\n",
        "\n",
        "    # Save the trained LabelEncoder for each column\n",
        "    pickle.dump(lbl, open(f'label_encoder_{column}.sav', 'wb'))"
      ],
      "metadata": {
        "id": "OvN_QFtksGKo"
      },
      "id": "OvN_QFtksGKo",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = [train, test, final_test]\n",
        "\n",
        "snap_columns = {\n",
        "    'CA': 'snap_CA',\n",
        "    'TX': 'snap_TX',\n",
        "    'WI': 'snap_WI'\n",
        "}\n",
        "\n",
        "# Loop over each dataset and apply the transformations\n",
        "for dataset in datasets:\n",
        "    for state_id, snap_col in snap_columns.items():\n",
        "        # Create the 'snap' column based on the state_id\n",
        "        dataset.loc[dataset['state_id'] == state_id, 'snap'] = dataset.loc[dataset['state_id'] == state_id][snap_col]\n",
        "\n",
        "    # Drop the original snap columns\n",
        "    dataset.drop(['snap_CA', 'snap_TX', 'snap_WI'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "Y2hcLwHeweI7"
      },
      "id": "Y2hcLwHeweI7",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframes = [train, test, final_test]\n",
        "\n",
        "columns_to_drop = ['weekday', 'wm_yr_wk']\n",
        "\n",
        "for df in dataframes:\n",
        "    df.drop(columns=columns_to_drop, axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "hTix71gUxXtj"
      },
      "id": "hTix71gUxXtj",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Engineering"
      ],
      "metadata": {
        "id": "zuHV1_QV3W7P"
      },
      "id": "zuHV1_QV3W7P"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "aa7a6edd",
      "metadata": {
        "id": "aa7a6edd"
      },
      "outputs": [],
      "source": [
        "def get_week_number(x):\n",
        "    date=calendar.datetime.date.fromisoformat(x)\n",
        "    return date.isocalendar()[1]\n",
        "\n",
        "train['week_number']=train['date'].apply(lambda x:get_week_number(x))\n",
        "test['week_number']=test['date'].apply(lambda x:get_week_number(x))\n",
        "final_test['week_number']=final_test['date'].apply(lambda x:get_week_number(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "407cef65",
      "metadata": {
        "id": "407cef65"
      },
      "outputs": [],
      "source": [
        "def get_season(x):\n",
        "    if x in [12,1,2]:\n",
        "        return 0      #\"Winter\"\n",
        "    elif x in [3,4,5]:\n",
        "        return 1   #\"Spring\"\n",
        "    elif x in [6,7,8]:\n",
        "        return 2   #\"Summer\"\n",
        "    else:\n",
        "        return 3   #\"Autumn\"\n",
        "\n",
        "\n",
        "train['season']=train['month'].apply(lambda x:get_season(x))\n",
        "test['season']=test['month'].apply(lambda x:get_season(x))\n",
        "final_test['season']=final_test['month'].apply(lambda x:get_season(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "37f27176",
      "metadata": {
        "id": "37f27176"
      },
      "outputs": [],
      "source": [
        "def check_if_quater_begin(x):\n",
        "    day=calendar.datetime.date.fromisoformat(x).day\n",
        "    month=calendar.datetime.date.fromisoformat(x).month\n",
        "    return 1 if (day==1 and (month in [1,4,7,9])) else 0\n",
        "\n",
        "\n",
        "train['quater_start']=train['date'].apply(lambda x:check_if_quater_begin(x))\n",
        "test['quater_start']=test['date'].apply(lambda x:check_if_quater_begin(x))\n",
        "final_test['quater_start']=final_test['date'].apply(lambda x:check_if_quater_begin(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "9e4f6140",
      "metadata": {
        "id": "9e4f6140"
      },
      "outputs": [],
      "source": [
        "def check_if_quater_end(x):\n",
        "    day=calendar.datetime.date.fromisoformat(x).day\n",
        "    month=calendar.datetime.date.fromisoformat(x).month\n",
        "    if (day==31 and month==3) or (day==30 and month==6) or (day==30 and month==9) or (day==31 and month==12):\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "\n",
        "train['quater_end']=train['date'].apply(lambda x:check_if_quater_end(x))\n",
        "test['quater_end']=test['date'].apply(lambda x:check_if_quater_end(x))\n",
        "final_test['quater_end']=final_test['date'].apply(lambda x:check_if_quater_end(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "51e345b3",
      "metadata": {
        "id": "51e345b3"
      },
      "outputs": [],
      "source": [
        "def month_start(x):\n",
        "    day=calendar.datetime.date.fromisoformat(x).day\n",
        "    return 1 if day==1 else 0\n",
        "\n",
        "\n",
        "train['month_start']=train['date'].apply(lambda x:month_start(x))\n",
        "test['month_start']=test['date'].apply(lambda x:month_start(x))\n",
        "final_test['month_start']=final_test['date'].apply(lambda x:month_start(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "defca9c7",
      "metadata": {
        "id": "defca9c7"
      },
      "outputs": [],
      "source": [
        "def month_end(x):\n",
        "    day=calendar.datetime.date.fromisoformat(x).day\n",
        "    month=calendar.datetime.date.fromisoformat(x).month\n",
        "    year=calendar.datetime.date.fromisoformat(x).year\n",
        "    leap_yr=(year%4==0)\n",
        "    val=(day==31 and month==1) or (day==29 if leap_yr else day==28) or (day==31 and month==3) or (day==30 and month==4) or\\\n",
        "        (day==31 and month==5) or (day==30 and month==6) or (day==31 and month==7) or (day==31 and month==8) or\\\n",
        "        (day==30 and month==9) or (day==31 and month==10) or (day==30 and month==11) or (day==31 and month==12)\n",
        "    return 1 if val else 0\n",
        "\n",
        "train['month_end']=train['date'].apply(lambda x:month_end(x))\n",
        "test['month_end']=test['date'].apply(lambda x:month_end(x))\n",
        "final_test['month_end']=final_test['date'].apply(lambda x:month_end(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "d6d07438",
      "metadata": {
        "id": "d6d07438"
      },
      "outputs": [],
      "source": [
        "def year_start(x):\n",
        "    day=calendar.datetime.date.fromisoformat(x).day\n",
        "    month=calendar.datetime.date.fromisoformat(x).month\n",
        "    return 1 if (day==1 and month==1) else 0\n",
        "\n",
        "train['year_start']=train['date'].apply(lambda x:year_start(x))\n",
        "test['year_start']=test['date'].apply(lambda x:year_start(x))\n",
        "final_test['year_start']=final_test['date'].apply(lambda x:year_start(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "10168b47",
      "metadata": {
        "id": "10168b47"
      },
      "outputs": [],
      "source": [
        "def year_end(x):\n",
        "    day=calendar.datetime.date.fromisoformat(x).day\n",
        "    month=calendar.datetime.date.fromisoformat(x).month\n",
        "    return 1 if (day==31 and month==12) else 0\n",
        "\n",
        "train['year_end']=train['date'].apply(lambda x:year_end(x))\n",
        "test['year_end']=test['date'].apply(lambda x:year_end(x))\n",
        "final_test['year_end']=final_test['date'].apply(lambda x:year_end(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "8a413e96",
      "metadata": {
        "id": "8a413e96"
      },
      "outputs": [],
      "source": [
        "cv=train[train['date']>='2016-03-28']\n",
        "train=train[train['date']<'2016-03-28']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "30b317a1",
      "metadata": {
        "id": "30b317a1"
      },
      "outputs": [],
      "source": [
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "d54f66b1",
      "metadata": {
        "id": "d54f66b1"
      },
      "outputs": [],
      "source": [
        "gc.collect()\n",
        "tt=pd.concat([train,cv,test,final_test])\n",
        "tt.sort_values(['id','date'],inplace=True)\n",
        "df=tt.pivot_table(index=['item_id','store_id'],columns='date',values='sales')\n",
        "df.fillna(0,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec193303",
      "metadata": {
        "scrolled": true,
        "id": "ec193303",
        "outputId": "a88efd14-aa2f-4a10-c855-7309d5d552a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature created named := roll_7_shift_28_mean\n",
            "Feature created named := roll_14_shift_28_mean\n",
            "Feature created named := roll_30_shift_28_mean\n",
            "Feature created named := roll_60_shift_28_mean\n",
            "Feature created named := roll_360_shift_28_mean\n",
            "Feature created named := roll_7_shift_28_std\n",
            "Feature created named := roll_14_shift_28_std\n",
            "Feature created named := roll_30_shift_28_std\n",
            "Feature created named := roll_60_shift_28_std\n",
            "Feature created named := roll_360_shift_28_std\n",
            "CPU times: total: 7min 47s\n",
            "Wall time: 11min 30s\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import gc\n",
        "\n",
        "def create_and_merge_rolling_features(df, dataframes, aggregate, shift, r):\n",
        "    \"\"\"\n",
        "    Create rolling feature and merge with given dataframes.\n",
        "\n",
        "    Args:\n",
        "    - df: DataFrame on which to calculate rolling features.\n",
        "    - dataframes: List of DataFrames to merge the rolling feature.\n",
        "    - aggregate: Aggregation function as a string.\n",
        "    - shift: Shift periods for rolling window.\n",
        "    - r: Rolling window size.\n",
        "    \"\"\"\n",
        "    # Calculate rolling features\n",
        "    roll = df.rolling(window=r, axis=1).agg(aggregate).shift(shift)\n",
        "    dates = roll.columns\n",
        "    name = f\"roll_{r}_shift_{shift}_{aggregate}\"\n",
        "    roll = roll.astype('float16')\n",
        "    roll.reset_index(level=[0, 1], inplace=True)\n",
        "    roll = pd.melt(roll, id_vars=['item_id', 'store_id'], value_vars=dates, var_name='date', value_name=name)\n",
        "    roll.fillna(-1, inplace=True)\n",
        "\n",
        "    # Merge with each dataframe in the list\n",
        "    for dataframe in dataframes:\n",
        "        dataframe.merge(roll, on=['item_id', 'store_id', 'date'], how='left')\n",
        "        print(f\"Feature created named:= {name}\")\n",
        "\n",
        "    # Cleanup\n",
        "    del roll\n",
        "    gc.collect()\n",
        "\n",
        "# Define parameters for loop\n",
        "aggregates = ['mean', 'std']\n",
        "shifts = [28]\n",
        "rolling_windows = [7, 14, 30, 60, 360]\n",
        "dataframes_to_merge = [train, cv, final_test, test]  # Assuming these are predefined\n",
        "\n",
        "# Loop over combinations of aggregates, shifts, and rolling windows\n",
        "for aggregate in aggregates:\n",
        "    for shift in shifts:\n",
        "        for r in rolling_windows:\n",
        "            create_and_merge_rolling_features(df, dataframes_to_merge, aggregate, shift, r)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_and_merge_ewm_feature(df, dataframes, shift_period, alpha_value, feature_name):\n",
        "    \"\"\"\n",
        "    Create Exponentially Weighted Moving Average (EWMA) feature and merge it with given dataframes.\n",
        "\n",
        "    Args:\n",
        "    - df: DataFrame from which to calculate the EWMA feature.\n",
        "    - dataframes: List of DataFrames to which the EWMA feature will be merged.\n",
        "    - shift_period: Periods to shift for calculation.\n",
        "    - alpha_value: Smoothing factor for EWMA.\n",
        "    - feature_name: Name of the created feature.\n",
        "    \"\"\"\n",
        "    # Shift and calculate EWMA\n",
        "    ewm_feature = df.shift(shift_period, axis=1).ewm(alpha=alpha_value, axis=1, adjust=False).mean()\n",
        "    ewm_feature = ewm_feature.astype('float16')\n",
        "    ewm_feature.reset_index(level=[0, 1], inplace=True)\n",
        "    ewm_feature = pd.melt(ewm_feature, id_vars=['item_id', 'store_id'], value_vars=ewm_feature.columns, var_name='date', value_name=feature_name)\n",
        "    ewm_feature.fillna(-1, inplace=True)\n",
        "\n",
        "    # Merge the feature with each DataFrame in the list\n",
        "    for dataframe in dataframes:\n",
        "        dataframe.merge(ewm_feature, on=['item_id', 'store_id', 'date'], how='left')\n",
        "        print(f\"Direct Feature created: {feature_name}\")\n",
        "\n",
        "# Assuming 'df', 'train', 'cv', 'test', and 'final_test' are predefined DataFrames\n",
        "dataframes_to_merge = [train, cv, test, final_test]\n",
        "\n",
        "# Call the function with specific parameters\n",
        "create_and_merge_ewm_feature(df=df, dataframes=dataframes_to_merge, shift_period=28, alpha_value=0.99, feature_name='direct_ewm')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33GHbUOp0cQJ",
        "outputId": "e070d13e-b5b8-4e8e-af10-172d82808779"
      },
      "id": "33GHbUOp0cQJ",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Direct Feature created: direct_ewm\n",
            "Direct Feature created: direct_ewm\n",
            "Direct Feature created: direct_ewm\n",
            "Direct Feature created: direct_ewm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c931ac38",
      "metadata": {
        "id": "c931ac38",
        "outputId": "bc640cb5-566f-4223-cd5d-2da1c1cdec5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature created for lag 28\n",
            "Feature created for lag 35\n",
            "Feature created for lag 42\n",
            "Feature created for lag 49\n",
            "Feature created for lag 56\n",
            "Feature created for lag 63\n",
            "Feature created for lag 70\n",
            "Feature created for lag 77\n",
            "Feature created for lag 84\n",
            "Feature created for lag 91\n",
            "Feature created for lag 98\n",
            "CPU times: total: 8min 21s\n",
            "Wall time: 12min 58s\n"
          ]
        }
      ],
      "source": [
        "def create_lag_feature(df, lag):\n",
        "    \"\"\"\n",
        "    Creates a lag feature for a given DataFrame and lag value.\n",
        "\n",
        "    Parameters:\n",
        "    - df: The DataFrame to shift.\n",
        "    - lag: The lag value.\n",
        "\n",
        "    Returns:\n",
        "    - A DataFrame containing the lag feature.\n",
        "    \"\"\"\n",
        "    i = 'direct_lag_' + str(lag)\n",
        "    lag_i = df.shift(lag, axis=1)\n",
        "    dates = lag_i.columns\n",
        "    lag_i.reset_index(level=[0, 1], inplace=True)\n",
        "    lag_i = pd.melt(lag_i, id_vars=['item_id', 'store_id'], value_vars=dates, var_name='date', value_name=i)\n",
        "    lag_i.fillna(-1, inplace=True)\n",
        "    lag_i[i] = lag_i[i].astype('int16')\n",
        "    return lag_i\n",
        "\n",
        "def merge_lag_feature(datasets, lag_feature):\n",
        "    \"\"\"\n",
        "    Merges the lag feature into the specified datasets.\n",
        "\n",
        "    Parameters:\n",
        "    - datasets: A list of DataFrames to merge the lag feature into.\n",
        "    - lag_feature: The lag feature DataFrame to merge.\n",
        "    \"\"\"\n",
        "    for i in range(len(datasets)):\n",
        "        datasets[i] = datasets[i].merge(lag_feature, on=['item_id', 'store_id', 'date'])\n",
        "\n",
        "# Assuming 'df' is your initial DataFrame and train, cv, test, final_test are defined\n",
        "for lag in range(28, 100, 7):\n",
        "    lag_feature = create_lag_feature(df, lag)\n",
        "    merge_lag_feature([train, cv, test, final_test], lag_feature)\n",
        "    print(\"Feature created for lag\", lag)\n",
        "    del lag_feature\n",
        "    gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccd68d7e",
      "metadata": {
        "id": "ccd68d7e",
        "outputId": "b0a80f41-fc20-4f8b-dcd7-7cdf0d9b9262"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 2min 52s\n",
            "Wall time: 6min 26s\n"
          ]
        }
      ],
      "source": [
        "# train.to_csv('train_df.csv',index=False)\n",
        "# cv.to_csv('cross_validation_df.csv',index=False)\n",
        "# test.to_csv('test_df.csv',index=False)\n",
        "# final_test.to_csv('final_test_df.csv',index=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}